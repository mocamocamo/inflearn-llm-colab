{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mocamocamo/inflearn-llm-colab/blob/main/4.%20Char_RNN%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_Language_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5Q0Ja1qlxM"
      },
      "source": [
        "# 필요한 라이브러리 import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz192HxKqlSK"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40vlolhCqqdv"
      },
      "source": [
        "# 유틸리티 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbQifXuxqs5d"
      },
      "source": [
        "# input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성하는 utility 함수를 정의합니다.\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "\n",
        "  return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJBc9ujquQ4"
      },
      "source": [
        "# 설정값 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap0uWvq9qvkH",
        "outputId": "9dcb8c27-159b-4c16-c432-bc9e77ae76cf"
      },
      "source": [
        "# 학습에 필요한 설정값들을 지정합니다.\n",
        "#data_dir = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')  # shakespeare\n",
        "data_dir = tf.keras.utils.get_file('linux.txt', 'https://raw.githubusercontent.com/solaris33/deep-learning-tensorflow-book-code/master/Ch08-RNN/Char-RNN/data/linux/input.txt')  # linux\n",
        "batch_size = 64      # Training : 64, Sampling : 1\n",
        "seq_length = 100     # Training : 100, Sampling : 1\n",
        "embedding_dim = 256  # Embedding 차원\n",
        "hidden_size = 1024   # 히든 레이어의 노드 개수\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/solaris33/deep-learning-tensorflow-book-code/master/Ch08-RNN/Char-RNN/data/linux/input.txt\n",
            "6209536/6206996 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGb_BMhqy8m"
      },
      "source": [
        "# 어휘 집합(Vocabulary set) 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEgzFH5Bq2AJ",
        "outputId": "86b6f674-29b8-4676-bee3-5dac15608e76"
      },
      "source": [
        "# 학습에 사용할 txt 파일을 읽습니다.\n",
        "text = open(data_dir, 'rb').read().decode(encoding='utf-8')\n",
        "# 학습데이터에 포함된 모든 character들을 나타내는 변수인 vocab과\n",
        "# vocab에 id를 부여해 dict 형태로 만든 char2idx를 선언합니다.\n",
        "vocab = sorted(set(text))  # 유니크한 character 개수\n",
        "vocab_size = len(vocab)\n",
        "print('{} unique characters'.format(vocab_size))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMkdoIeAq5DJ"
      },
      "source": [
        "# Dataset 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N57em_CFrAwG"
      },
      "source": [
        "# 학습 데이터를 character에서 integer로 변환합니다.\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# split_input_target 함수를 이용해서 input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성합니다.\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGraOEVirC1w"
      },
      "source": [
        "# RNN 모델 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgUcylhQrFgX"
      },
      "source": [
        "# tf.keras.Model을 이용해서 RNN 모델을 정의합니다.\n",
        "class RNN(tf.keras.Model):\n",
        " def __init__(self, batch_size):\n",
        "   super(RNN, self).__init__()\n",
        "   self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                    batch_input_shape=[batch_size, None])\n",
        "   self.hidden_layer_1 = tf.keras.layers.LSTM(hidden_size,\n",
        "                                             return_sequences=True,\n",
        "                                             stateful=True,\n",
        "                                             recurrent_initializer='glorot_uniform')\n",
        "   self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        " def call(self, x):\n",
        "   embedded_input = self.embedding_layer(x)\n",
        "   features = self.hidden_layer_1(embedded_input)\n",
        "   logits = self.output_layer(features)\n",
        "\n",
        "   return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZlqa1SrGjL"
      },
      "source": [
        "# Loss Function 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzTsknETrJQC"
      },
      "source": [
        "# sparse cross-entropy 손실 함수를 정의합니다.\n",
        "def sparse_cross_entropy_loss(labels, logits):\n",
        "  return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7_1vbG0rMW9"
      },
      "source": [
        "# 옵티마이저 및 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMYAASYrOQi"
      },
      "source": [
        "# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 최적화를 위한 function을 정의합니다.\n",
        "@tf.function\n",
        "def train_step(model, input, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(input)\n",
        "    loss = sparse_cross_entropy_loss(target, logits)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3E2PRvnrSeP"
      },
      "source": [
        "# 샘플링 함수 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgeX0b6HrRyA"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다.\n",
        "\n",
        "  # start_sting을 integer 형태로 변환합니다.\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 샘플링 결과로 생성된 string을 저장할 배열을 초기화합니다.\n",
        "  text_generated = []\n",
        "\n",
        "  # 낮은 temperature 값은 더욱 정확한 텍스트를 생성합니다.\n",
        "  # 높은 temperature 값은 더욱 다양한 텍스트를 생성합니다.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기서 batch size = 1 입니다.\n",
        "  model.reset_states()\n",
        "  for i in range(num_sampling):\n",
        "    predictions = model(input_eval)\n",
        "    # 불필요한 batch dimension을 삭제합니다.\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # 모델의 예측결과에 기반해서 랜덤 샘플링을 하기위해 categorical distribution을 사용합니다.\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # 예측된 character를 다음 input으로 사용합니다.\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    # 샘플링 결과를 text_generated 배열에 추가합니다.\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2krRKnbvr00T"
      },
      "source": [
        "# 트레이닝 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYxCGdZvoYHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a5d531-eb1b-4126-d0f3-3105321a6a4c"
      },
      "source": [
        "# Recurrent Neural Networks(RNN) 모델을 선언합니다.\n",
        "RNN_model = RNN(batch_size=batch_size)\n",
        "\n",
        "# 데이터 구조 파악을 위해서 예제로 임의의 하나의 배치 데이터 에측하고, 예측결과를 출력합니다.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = RNN_model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# 모델 정보를 출력합니다.\n",
        "RNN_model.summary()\n",
        "\n",
        "# checkpoint 데이터를 저장할 경로를 지정합니다.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start = time.time()\n",
        "\n",
        "  # 매 반복마다 hidden state를 초기화합니다. (최초의 hidden 값은 None입니다.)\n",
        "  hidden = RNN_model.reset_states()\n",
        "\n",
        "  for (batch_n, (input, target)) in enumerate(dataset):\n",
        "    loss = train_step(RNN_model, input, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 5회 반복마다 파라미터를 checkpoint로 저장합니다.\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "print(\"트레이닝이 끝났습니다!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 99) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"rnn_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  25344     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  101475    \n",
            "=================================================================\n",
            "Total params: 5,373,795\n",
            "Trainable params: 5,373,795\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 Batch 0 Loss 4.595424652099609\n",
            "Epoch 1 Batch 100 Loss 2.8029003143310547\n",
            "Epoch 1 Batch 200 Loss 2.368008613586426\n",
            "Epoch 1 Batch 300 Loss 1.9701776504516602\n",
            "Epoch 1 Batch 400 Loss 1.740452527999878\n",
            "Epoch 1 Batch 500 Loss 1.6691888570785522\n",
            "Epoch 1 Batch 600 Loss 1.5763310194015503\n",
            "Epoch 1 Batch 700 Loss 1.5131900310516357\n",
            "Epoch 1 Batch 800 Loss 1.4898241758346558\n",
            "Epoch 1 Batch 900 Loss 1.3949942588806152\n",
            "Epoch 1 Loss 1.3979\n",
            "Time taken for 1 epoch 64.18272733688354 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8904311656951904\n",
            "Epoch 2 Batch 100 Loss 1.4212713241577148\n",
            "Epoch 2 Batch 200 Loss 1.387321949005127\n",
            "Epoch 2 Batch 300 Loss 1.2630565166473389\n",
            "Epoch 2 Batch 400 Loss 1.1293050050735474\n",
            "Epoch 2 Batch 500 Loss 1.3102813959121704\n",
            "Epoch 2 Batch 600 Loss 1.2361220121383667\n",
            "Epoch 2 Batch 700 Loss 1.237483263015747\n",
            "Epoch 2 Batch 800 Loss 1.2400585412979126\n",
            "Epoch 2 Batch 900 Loss 1.20728600025177\n",
            "Epoch 2 Loss 1.1243\n",
            "Time taken for 1 epoch 64.56112098693848 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2513527870178223\n",
            "Epoch 3 Batch 100 Loss 1.125900387763977\n",
            "Epoch 3 Batch 200 Loss 1.1476699113845825\n",
            "Epoch 3 Batch 300 Loss 1.123602271080017\n",
            "Epoch 3 Batch 400 Loss 1.0869883298873901\n",
            "Epoch 3 Batch 500 Loss 1.0950208902359009\n",
            "Epoch 3 Batch 600 Loss 1.1275570392608643\n",
            "Epoch 3 Batch 700 Loss 1.195296049118042\n",
            "Epoch 3 Batch 800 Loss 1.0688831806182861\n",
            "Epoch 3 Batch 900 Loss 1.0584933757781982\n",
            "Epoch 3 Loss 1.0435\n",
            "Time taken for 1 epoch 81.9296350479126 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.1756036281585693\n",
            "Epoch 4 Batch 100 Loss 1.0549757480621338\n",
            "Epoch 4 Batch 200 Loss 1.0179253816604614\n",
            "Epoch 4 Batch 300 Loss 1.0016586780548096\n",
            "Epoch 4 Batch 400 Loss 1.034349799156189\n",
            "Epoch 4 Batch 500 Loss 1.0481075048446655\n",
            "Epoch 4 Batch 600 Loss 1.0366021394729614\n",
            "Epoch 4 Batch 700 Loss 1.0327637195587158\n",
            "Epoch 4 Batch 800 Loss 0.959072470664978\n",
            "Epoch 4 Batch 900 Loss 1.0204027891159058\n",
            "Epoch 4 Loss 1.0377\n",
            "Time taken for 1 epoch 68.18256163597107 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.015885591506958\n",
            "Epoch 5 Batch 100 Loss 1.0565637350082397\n",
            "Epoch 5 Batch 200 Loss 0.992794930934906\n",
            "Epoch 5 Batch 300 Loss 1.0025701522827148\n",
            "Epoch 5 Batch 400 Loss 0.9452717304229736\n",
            "Epoch 5 Batch 500 Loss 0.8931390643119812\n",
            "Epoch 5 Batch 600 Loss 0.9416240453720093\n",
            "Epoch 5 Batch 700 Loss 1.050402045249939\n",
            "Epoch 5 Batch 800 Loss 0.9717079401016235\n",
            "Epoch 5 Batch 900 Loss 0.9858691692352295\n",
            "Epoch 5 Loss 0.9969\n",
            "Time taken for 1 epoch 67.7448480129242 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.999776303768158\n",
            "Epoch 6 Batch 100 Loss 0.9565187096595764\n",
            "Epoch 6 Batch 200 Loss 0.8619758486747742\n",
            "Epoch 6 Batch 300 Loss 0.8680692911148071\n",
            "Epoch 6 Batch 400 Loss 0.848659873008728\n",
            "Epoch 6 Batch 500 Loss 0.9475144743919373\n",
            "Epoch 6 Batch 600 Loss 0.925066351890564\n",
            "Epoch 6 Batch 700 Loss 0.9546129703521729\n",
            "Epoch 6 Batch 800 Loss 0.9545434713363647\n",
            "Epoch 6 Batch 900 Loss 0.881976306438446\n",
            "Epoch 6 Loss 0.9614\n",
            "Time taken for 1 epoch 67.80332779884338 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.013685703277588\n",
            "Epoch 7 Batch 100 Loss 0.9058746099472046\n",
            "Epoch 7 Batch 200 Loss 0.9408100247383118\n",
            "Epoch 7 Batch 300 Loss 0.8668063282966614\n",
            "Epoch 7 Batch 400 Loss 0.9071646332740784\n",
            "Epoch 7 Batch 500 Loss 0.8915481567382812\n",
            "Epoch 7 Batch 600 Loss 0.918425440788269\n",
            "Epoch 7 Batch 700 Loss 0.9360130429267883\n",
            "Epoch 7 Batch 800 Loss 0.8834318518638611\n",
            "Epoch 7 Batch 900 Loss 0.9235985279083252\n",
            "Epoch 7 Loss 0.8838\n",
            "Time taken for 1 epoch 68.06172704696655 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.008132815361023\n",
            "Epoch 8 Batch 100 Loss 0.9254401922225952\n",
            "Epoch 8 Batch 200 Loss 0.921670138835907\n",
            "Epoch 8 Batch 300 Loss 0.8934357166290283\n",
            "Epoch 8 Batch 400 Loss 0.809149980545044\n",
            "Epoch 8 Batch 500 Loss 0.9418428540229797\n",
            "Epoch 8 Batch 600 Loss 0.8931186199188232\n",
            "Epoch 8 Batch 700 Loss 0.8268086314201355\n",
            "Epoch 8 Batch 800 Loss 0.8832077980041504\n",
            "Epoch 8 Batch 900 Loss 0.8943663239479065\n",
            "Epoch 8 Loss 0.8802\n",
            "Time taken for 1 epoch 68.04621863365173 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8887687921524048\n",
            "Epoch 9 Batch 100 Loss 0.8788896203041077\n",
            "Epoch 9 Batch 200 Loss 0.8118340969085693\n",
            "Epoch 9 Batch 300 Loss 0.8165507316589355\n",
            "Epoch 9 Batch 400 Loss 0.858704149723053\n",
            "Epoch 9 Batch 500 Loss 0.8680853843688965\n",
            "Epoch 9 Batch 600 Loss 0.8612232208251953\n",
            "Epoch 9 Batch 700 Loss 0.8178400993347168\n",
            "Epoch 9 Batch 800 Loss 0.9306390285491943\n",
            "Epoch 9 Batch 900 Loss 0.8305807709693909\n",
            "Epoch 9 Loss 0.8791\n",
            "Time taken for 1 epoch 67.58101749420166 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8652300238609314\n",
            "Epoch 10 Batch 100 Loss 0.8335866332054138\n",
            "Epoch 10 Batch 200 Loss 0.8167266845703125\n",
            "Epoch 10 Batch 300 Loss 0.8440757989883423\n",
            "Epoch 10 Batch 400 Loss 0.8192751407623291\n",
            "Epoch 10 Batch 500 Loss 0.8481991291046143\n",
            "Epoch 10 Batch 600 Loss 0.9068182110786438\n",
            "Epoch 10 Batch 700 Loss 0.8095198273658752\n",
            "Epoch 10 Batch 800 Loss 0.7869399785995483\n",
            "Epoch 10 Batch 900 Loss 0.8212043642997742\n",
            "Epoch 10 Loss 0.8231\n",
            "Time taken for 1 epoch 67.14899373054504 sec\n",
            "\n",
            "트레이닝이 끝났습니다!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un7SxSy9r6j0"
      },
      "source": [
        "# 트레이닝이 끝난 모델을 이용한 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL5aDi4Ioaz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a40bd5-c215-4900-c060-79ebe4925f77"
      },
      "source": [
        "sampling_RNN_model = RNN(batch_size=1)\n",
        "sampling_RNN_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "sampling_RNN_model.build(tf.TensorShape([1, None]))\n",
        "sampling_RNN_model.summary()\n",
        "\n",
        "# 샘플링을 시작합니다.\n",
        "print(\"샘플링을 시작합니다!\")\n",
        "print(generate_text(sampling_RNN_model, start_string=u' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"rnn_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      multiple                  25344     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  101475    \n",
            "=================================================================\n",
            "Total params: 5,373,795\n",
            "Trainable params: 5,373,795\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "샘플링을 시작합니다!\n",
            " ss           printf(\" .. MAX_LOCK_USASHIC_PRINT_TASK_TIMEKEEntry))\n",
            "\t\tif (diag) {\n",
            "\t\treset_free_percpu(skb);\n",
            "\t\tagain_ops->proc_cap_buffer = bytesn;\n",
            "\t}\n",
            "\n",
            "\t/*\n",
            "\t * Determine if CONFIG_SCHEDSTATS */\n",
            "\n",
            "\t/* Keep start */\n",
            "\tdr->flags &= ~SPONT_PAREN; i++) {\n",
            "\t\tkdb_printf(\"kdb enabled\" by this code trigger that may have been actually. If all\n",
            " * threads can possibly get_kprojid to insert a CPU with swsusp_bit)\n",
            "\t\t.binary);\n",
            "\t}\n",
            "\tkdb_register_flags(_m);\n",
            "\t__add_notify(dest);\n",
            "}\n",
            "\n",
            "static DEFINE_MUTEX(entry_count\" },\n",
            "\t{ CTL_INT,\tFS_REAL,\t\t\"pall\" },\n",
            "\t/* KERN_s reschedule if verified to etc. Watchdog namespace\n",
            "\t * data is not yet, disable installs we need multiple.  Note:\n",
            "\t * See prepare_creds() values in a sock when might be\n",
            "\t * been the timexw->goto out;\n",
            "\tr_start and characters. */\n",
            "static int show_rcubarrierdev(struct request_queue *q, struct rw_semaphore *seturn -EINVAL one reads */\n",
            "\tif (isdigit(TASKLET_STATE_COMING &&\n",
            "\t    state == KDB_CMD_CPU) {\n",
            "\t\tif (atomic_inc_retitid == CONFIG_MODULE_NO_BRL_0) {\n",
            "\t\tkdb_printf(\"%s: at syscall audit message or not all of system callbacks while shorted event\n",
            "\t\t * instruction within we check the\n",
            "\t\t\t * callback is the offset of arch pages were above.\n",
            "\t\t\t */\n",
            "\t\t\tif (nextarg > name)\n",
            "\t\t\t\tlen += kprobe_dit(nlst, userstromph_write,\n",
            "\t\t\t\t&mod->mkobE_SHIFT,\n",
            "\t\t\trdp->nocb_head);\n",
            "\tup_read(&mm->mmareeder.\n",
            " */\n",
            "static void blk_add_trace_rq_insert(struct ring_buffer_percpu *cpu_buffer = buffer->buffers;\n",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;\n",
            "\tunsigned long pfn = res->flags;\n",
            "\n",
            "\t/*\n",
            "\t * Unbind are files. The safe if there is nothing towards the list, it has already\n",
            "\t * stop may canclude <linux/kmod.h>\n",
            "#include <linux/shareded.h>\n",
            "#include <linux/proc_fs.h>\n",
            "#include <linux/fs.h>\n",
            "#include <linux/pidfcer.h>\n",
            "#include <linux/binfmtx].mutex);\n",
            "}\n",
            "EXPORT_SYMBOL(add_taint);\n",
            "\n",
            "static void kallsyms_lookup(unsigned long ip, unsigned long *;\n",
            "\n",
            "\tstruct seq_file *mit;\n",
            "\tint ret;\n",
            "\n",
            "\tif (!file)\n",
            "\t\toptions = true;\n",
            "}\n",
            "/*\n",
            " * get_parent - Free a given event that,\n",
            " * second for all kthread_cpu - atomically allocate and modify\n",
            " * 8, 1916 -  success - crc = -1, ns\n",
            " * an option is placed, it out of them symbol and parsing entries with\n",
            " *\ttrace-issues.\n",
            " * @line: The image hierarchies on correctned by Nick Joss, ArjaN wereo);\n",
            "\n",
            "\tif (nr_late != header && flags || !pre_mask)\n",
            "\t\tgoto error;\n",
            "\n",
            "\tbt->nr_cpu_ids; i++) {\n",
            "\t\tstatic int run_read_unlock_sched_timer(struct ptrace_remove_work aterator to start checkevent_utr_task(struct rt_mutex *lock)\n",
            "{\n",
            "\treturn __alarm_base_files(old, callchain_must_stric, current);\n",
            "}\n",
            "\n",
            "user_resource(r, p);\n",
            "\t\telse\n",
            "\t\t\treset_iter(&bpage->elements);\n",
            "\t\tbcfs_validate_change(char *cmdline,\n",
            "\t\t\t\t     unsigned long address,\n",
            "\t\t\t\tunsigned long min_sze,\n",
            "\t\t\t        char\t*bufptr, is_read,\n",
            "\t\tstruct blk_irq_bw *blk_add_trace_rq, int permission== buf_addr)\n",
            "{\n",
            "\tstruct module_kobject *mk;\n",
            "\tint cpu;\n",
            "\n",
            "\tpr_devel(\"<%s\\n\", __enterisers - ring buffer's process or already confindings.h\"\n",
            " \"write: {\n",
            "\tLOGG_COUNT;\n",
            "\tpr_info(\"\\t%d %ld\", pid_q bin[i], \\\n",
            "\t\t\t\t     next_page, *respage), PAGE_SIZE))\n",
            "\t\t\tavail = strchr(sp, right);\n",
            "\t\tbit(val, &val), event) defined(CONFIG_MMU\n",
            "\t{\n",
            "\t\tstruct task_numa_free directory parsed;\n",
            "\t\t\tif (!s->ss->cfs_cape.start_lba\n",
            "\t\tredister_sysctl();\n",
            "\n",
            "\t\tswitch (c->type) {\n",
            "\t\t\tif (ressize(&right = frozen - low_fetch_irq)(SU_DESTREAD_ALLOW, &root_desc);\n",
            "\t\tif (rdtp->dynticks_idle_nestint\t= 0;\n",
            "\t\t\t\t\t\t\t\t    RLIME_NFP fmt;\n",
            "\tarm_timer(timr->it_i_uprobe_buffer, 1);\n",
            "\t\t\telta trace_add_unbid\n",
            "\t\tS_INTMASK,\t\t\"ip_opport\n",
            "\t\t * disarm entries:\n",
            "\t\t\t*/\n",
            "\t\t\tif (argc != 1\n",
            "\t    && oms[0]) {\n",
            "\t\t\tif (!te_cpu_base->cpumask.cbcpu)\n",
            "\t\t\t\tbreak;\n",
            "\t\t}\n",
            "\t\tacct_acquire(nval, &val);\n",
            "\t\treturn TRACE_TYPE_HASH_BITS;\n",
            "\t}\n",
            "\n",
            "\t/* Check synchronize installs, but we assume missing a trace probe */\n",
            "\tproc_watchdog();\n",
            "\tkthread to the new kernel process.\n",
            " *\n",
            " * @start: start address\n",
            " * @arg:\targv[3]\n",
            "\tmodule_usecsize 16:\n",
            "\t\tdisable_irq = 0;\n",
            "\t} while (read_seqcount_ble projes starts bA.\n",
            "\t */\n",
            "\tcopied = true;\n",
            "\telse if ((strctx->flags & KEXEC_FILE) &&\n",
            "\t      elemenable)\n",
            "\t\t\talarm->node = attr->addr;\n",
            "\t\tbreak;\n",
            "\tcase Audit_block from us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsimgaIDG0-r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}