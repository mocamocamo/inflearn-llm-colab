{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMo6OsNcK6MoSRbqD62kLsz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mocamocamo/inflearn-llm-colab/blob/main/3_N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram 예제\n",
        "- Reference : https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
        "\n",
        "## NLTK 라이브러리 설치 및 import"
      ],
      "metadata": {
        "id": "UBNA7DcwmAWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH3PwDOik3xs",
        "outputId": "8dcf7307-ac18-4127-f969-96bb42227da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten"
      ],
      "metadata": {
        "id": "Lpr6wlS5k418"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\", quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSPqhABnlA41",
        "outputId": "25edf414-b180-4eff-dccd-e493580d211a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['apple', 'grape', 'carrot'], ['apple', 'carrot', 'melon', 'grape', 'melon', 'watermelon']]"
      ],
      "metadata": {
        "id": "zeU9O1mElDxc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(text[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS0TsCjBlUtr",
        "outputId": "d129834c-62f6-491b-b099-e51af9636363"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apple', 'grape'), ('grape', 'carrot')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(text[1], n=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_7coltlXWn",
        "outputId": "9f51c620-05f1-4aa9-a828-59676c559dfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apple', 'carrot', 'melon'),\n",
              " ('carrot', 'melon', 'grape'),\n",
              " ('melon', 'grape', 'melon'),\n",
              " ('grape', 'melon', 'watermelon')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장의 시작(\\<s>\\)과 끝(\\</s>\\)을 나타내는 Padding 추가"
      ],
      "metadata": {
        "id": "pe8NH7OomMNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAnVaI1jlZxo",
        "outputId": "40a99803-3aaf-4b77-b6fc-f8ccdd5a2c53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'apple', 'grape', 'carrot', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sent = list(pad_sequence(text[0],\n",
        "                                pad_left=True, left_pad_symbol=\"<s>\",\n",
        "                                pad_right=True, right_pad_symbol=\"</s>\",\n",
        "                                n=2))\n",
        "list(ngrams(padded_sent, n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH52N1HOlp3O",
        "outputId": "b66fe9bf-1898-4ff2-d4d3-1d92c233b506"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'apple'), ('apple', 'grape'), ('grape', 'carrot'), ('carrot', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pad_both_ends 함수를 이용해서 이 과정을 좀더 쉽게 수행할 수 있습니다."
      ],
      "metadata": {
        "id": "6aZOld8WmRC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "list(pad_both_ends(text[0], n=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thzbA8RJlxrL",
        "outputId": "fd10b94f-c4e7-421d-88f0-e8682a6b0ff7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'apple', 'grape', 'carrot', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(pad_both_ends(text[0], n=2)))"
      ],
      "metadata": {
        "id": "-y0KLINLmg3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a7afc8-b1e6-4b4d-f82b-5d7c06e95065"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'apple'), ('apple', 'grape'), ('grape', 'carrot'), ('carrot', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# everygrams 함수를 이용해서 각 N-gram(e.g. 1-gram, 2-gram, 3-gram)의 시작과 끝에 padding을 적용할 수 있습니다."
      ],
      "metadata": {
        "id": "28rOgcWWpc9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import everygrams\n",
        "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
        "print(padded_bigrams)\n",
        "print(list(everygrams(padded_bigrams, max_len=3)))"
      ],
      "metadata": {
        "id": "b9uRC1BVmV9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250da43b-ed32-487e-c70d-b94e3ec7af03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'apple', 'grape', 'carrot', '</s>']\n",
            "[('<s>',), ('<s>', 'apple'), ('<s>', 'apple', 'grape'), ('apple',), ('apple', 'grape'), ('apple', 'grape', 'carrot'), ('grape',), ('grape', 'carrot'), ('grape', 'carrot', '</s>'), ('carrot',), ('carrot', '</s>'), ('</s>',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# flatten 함수를 이용해서 모든 문자들을 펼칠 수 있습니다."
      ],
      "metadata": {
        "id": "NhVAEMVQpXh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import flatten\n",
        "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9TdxDU3pLDe",
        "outputId": "8a69aa88-81d0-491e-d94d-129d63c5228f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'apple',\n",
              " 'grape',\n",
              " 'carrot',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'apple',\n",
              " 'carrot',\n",
              " 'melon',\n",
              " 'grape',\n",
              " 'melon',\n",
              " 'watermelon',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
        "for ngramlize_sent in training_ngrams:\n",
        "  print(f\"ngramlize_sent: {list(ngramlize_sent)}\")\n",
        "\n",
        "list(padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH1NjDRZpUld",
        "outputId": "1e3fdf94-fa32-4ea9-a757-e3754b55626d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngramlize_sent: [('<s>',), ('<s>', 'apple'), ('apple',), ('apple', 'grape'), ('grape',), ('grape', 'carrot'), ('carrot',), ('carrot', '</s>'), ('</s>',)]\n",
            "ngramlize_sent: [('<s>',), ('<s>', 'apple'), ('apple',), ('apple', 'carrot'), ('carrot',), ('carrot', 'melon'), ('melon',), ('melon', 'grape'), ('grape',), ('grape', 'melon'), ('melon',), ('melon', 'watermelon'), ('watermelon',), ('watermelon', '</s>'), ('</s>',)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'apple',\n",
              " 'grape',\n",
              " 'carrot',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'apple',\n",
              " 'carrot',\n",
              " 'melon',\n",
              " 'grape',\n",
              " 'melon',\n",
              " 'watermelon',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 테스트를 위한 텍스트 파일(language-never-random.txt)을 다운받습니다."
      ],
      "metadata": {
        "id": "qhrQZfnMqIGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
        "if os.path.isfile('language-never-random.txt'):\n",
        "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
        "        text = fin.read()\n",
        "else:\n",
        "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "    text = requests.get(url).content.decode('utf8')\n",
        "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(text)\n",
        "\n",
        "\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "ffCp-zDFprgt",
        "outputId": "8588cc36-8f3f-4db1-d5d9-7a6c28cc4bee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                       Language is never, ever, ever, random\\n\\n                                                               ADAM KILGARRIFF\\n\\n\\n\\n\\nAbstract\\nLanguage users never choose words randomly, and language is essentially\\nnon-random. Statistical hypothesis testing uses a null hypothesis, which\\nposits randomness. Hence, when we look at linguistic phenomena in cor-\\npora, the null hypothesis will never be true. Moreover, where there is enough\\ndata, we shall (almost) always be able to establish that it is not true. In\\ncorpus studies, we frequently do have enough data, so the fact that a rela-\\ntion between two phenomena is demonstrably non-random, does not sup-\\nport the inference that it is not arbitrary. We present experimental evidence\\nof how arbitrary associations between word frequencies and corpora are\\nsystematically non-random. We review literature in which hypothesis test-\\ning has been used, and show how it has often led to unhelpful or mislead-\\ning results.\\nKeywords: 쎲쎲쎲\\n\\n1. Introduction\\nAny two phenomena might or might not be related. The range of pos-\\nsibilities is that the association is Random, Arbitrary, Motivated or Pre-\\ndictable (R, A, M, P). The bulk of linguistic questions concern the dis-\\ntinction between A and M. A linguistic account of a phenomenon gen-\\nerally gives us reason to view the relation between, for example, a verb’s\\nsyntax and its semantics, as motivated rather than arbitrary. However,\\nit is not in general possible to model the A-M distinction mathematically.\\nThe distinction that can be modeled mathematically is between R and\\nnot-R, that is, between random, or uncorrelated, pairs and pairs where\\nthere is some correlation, be it arbitrary, motivated or predictable.1 The\\nmechanism here is hypothesis-testing. A null hypothesis, H0 is con-\\nstructed to model the situation in which there is no correlation between\\n\\nCorpus Linguistics and Linguistic Theory 1⫺2 (2005), 263⫺275     1613-7027/05/0001⫺0263\\n                                                                     쑕 Walter de Gruyter\\n\\x0c264    A. Kilgarriff\\n\\nthe two phenomena. As the mathematics of the random is well under-\\nstood, we can compute the likelihood of the null hypothesis given the\\ndata. If the likelihood is low, we reject H0.\\n   The problem for empirical linguistics is that language is not random,\\nso the null hypothesis is never true. Language is not random because we\\nspeak or write with purposes. We do not, indeed, without computational\\nhelp are not capable of, producing words or sounds or sentences or\\ndocuments randomly. We do not always have enough data to reject the\\nnull hypothesis, but that is a distinct issue: wherever there is enough\\ndata, it is rejected. Using language corpora, we are frequently in the\\nfortunate position of having very large quantities of data at our disposal.\\nThen, even where pairs of corpora are set up to be linguistically identical,\\nthe null hypothesis is resoundingly defeated. In section 4, we present an\\nexperiment demonstrating this counterintuitive effect.\\n   There are a number of papers in the empirical linguistics literature\\nwhere researchers seemed to be testing whether an association was lin-\\nguistically salient, or used the confidence with which H0 could be re-\\njected as a measure of salience, whereas in fact they were merely testing\\nwhether they had enough data to reject H0 with confidence. Some such\\ncases are reviewed in section 5. Hypothesis testing has been widely used\\nin the acquisition of subcategorization frames from corpora and this\\nliterature is considered in some detail. Alternatives to inappropriate hy-\\npothesis-testing are presented.\\n   Before proceeding, may I clarify that this paper is in no way critical\\nof using probability models, all of which are based on assumptions of\\nrandomness, in empirical linguistics in general. Probability models have\\nbeen responsible for a large share of progress in the field in the last\\ndecade and a half. The randomness assumptions are always untrue, but\\nthat does not preclude them from frequently being useful. Making false\\nassumptions is often an ingenious way to proceed; the problem arises\\nwhere the literal falsity of the assumption is overlooked, and inappropri-\\nate inferences are drawn.\\n\\n2. The arbitrary and the random\\nIn common parlance, random and arbitrary are synonyms, with diction-\\naries giving near-identical definitions: LDOCE (1995) defines random as\\n  happening or chosen without any definite plan, or pattern\\nand arbitrary as\\n  1 decided or arranged without any reason or plan, often unfairly … 2\\n  happening or decided by chance rather than a plan\\n\\x0c                              Language is never, ever, ever, random     265\\n\\n   Superficially, randomness, as defined here, is what the technical sense\\nof random captures and makes explicit. The technical sense is defined in\\nterms of statistical independence. First, we formalize the framework:\\n\\n  For a population of events, the first phenomenon holds where x is\\n  true of the event, the second holds where y is true of the event.\\n\\n   Now, the relation between the phenomena is random iff the prob-\\nability of x, for that subset of events where y does hold, is identical to\\nits probability for the subset where y does not hold, that is\\n\\n  P (x|y) ⫽ P (x|ÿ y)\\n\\n   The relation is symmetric: P (x|y) ⫽ P (x|ÿ y) entails P (y|x) ⫽\\nP (y|ÿ x). Hereafter I use ‘random’ for the technical meaning and ‘arbi-\\ntrary’ for the non-technical one.\\n   Arbitrary events are very rarely random, and random events are very\\nrarely arbitrary. It takes considerable ingenuity and sophisticated mathe-\\nmatics to produce a pseudo-random sequence algorithmically, and true\\nrandomness is not possible at all. Events happening “without any defi-\\nnite plan, aim or pattern” are, by definition, arbitrary, but are vanish-\\ningly unlikely to be random. Outside the sub-atomic realm, natural\\nevents are very rarely random.\\n   Consider, for example, cat food purchases and shoe-polish purchases\\nwithin the space of all UK supermarket-shopping events: does the fact\\nthat cat food was bought predict (positively or negatively) whether shoe\\npolish was bought in the same shopping trip? There is no obvious reason\\nwhy it should, and we can happily declare the relation arbitrary. But\\nperhaps either cat food or shoe-polish are more (or less) often bought in\\nhot (or cold) weather, or on Saturday nights, or Sunday mornings, or\\nMonday lunchtimes, or by richer (or poorer) people, or by men (or\\nwomen), or by people in (or out of) towns… There is an unlimited\\nnumber of hypotheses connecting the two (positively or negatively); if\\njust one of these has any validity, however weak, then the null hypothesis\\nis false.\\n   At this point, you may question why the null hypothesis is ever a\\nuseful construct.\\n   For a wide range of tasks, although H0 is false, there is only enough\\nevidence to establish the fact if there is a strong relation between the two\\nphenomena. Thus, given evidence from 1,000 shopping trips, it is un-\\nlikely we shall be able to reject H0 concerning cat food and shoe-polish,\\nwhereas we shall be able to reject it concerning strawberry-buying and\\ncream-buying. Given further evidence, perhaps from 1,000,000 shopping\\n\\x0c266    A. Kilgarriff\\n\\ntrips, we shall also be able to reject the null hypothesis regarding nappy2-\\nbuying and beer-sixpack-buying. (The correlation, the most newsworthy\\nproduct of large-scale data mining by supermarkets, was widely reported\\nin the British media.) But still not for catf ood and shoe-polish. But,\\ngiven 1,000,000,000 events, we shall in all likelihood also be able to reject\\nit for cat food and shoe-polish.\\n   Whether or not we can reject the null hypothesis (with eg. 95 % confi-\\ndence) is a function of sample size and level of correlation. Where sample\\nsize is held constant (and is not enormous), whether or not we can reject\\nH0 can be seen as a way of providing statistical support for distinguish-\\ning the arbitrary and the motivated. This is a role that hypothesis testing\\nplays across the social sciences. However where the sample size varies\\nby an order of magnitude, or where it is enormous, it is wrong to identify\\nthe accept-H0/reject-H0 distinction with the arbitrary/motivated one.\\n   The uneasy relationship between hypothesis-testing, and quantity of\\ndata, is familiar to statisticians though frequently overlooked or mis-\\nunderstood by users of statistics (Carver 1993, Stubbs 1995, Brandstätter\\n1999). One statistics textbook warns thus:\\n  None of the null hypotheses we have considered with respect to good-\\n  ness of fit can be exactly true, so if we increase the sample size (and\\n  hence the value of χ2) we would ultimately reach the point when all\\n  null hypotheses would be rejected. All that the χ2 test can tell us, then,\\n  is that the sample size is too small to reject the null hypothesis! (Owen\\n  and Jones, 1977, p 359).\\nThe issue is particularly salient for empirical linguistics because, firstly,\\nwe have access to extremely large sample sizes, and secondly, the distri-\\nbution of many language phenomena is Zipfian. The has 6,000,000 oc-\\ncurrences in the BNC whereas cat food (spelled as one word or two) has\\n66. For a vast number of third phenomena X, the null hypothesis that\\nthe and X are uncorrelated will be rejected, whereas the null hypothesis\\nthat cat food and X are uncorrelated will not. It would be wrong to draw\\ninferences about what was arbitrary, what motivated.\\n\\n3. Objections to Maximum Likelihood Estimates (MLEs)\\nChurch and Hanks (1990) inaugurated the research area of lexical statis-\\ntics with their presentation of Mutual Information (I), a measure of how\\nclosely associated two phenomena are. It can be applied to finding words\\nwhich occur together to a noteworthy degree, or to finding words which\\nare particularly associated with one corpus as against another, or for\\nvarious other purposes.3 They define the mutual information between\\ntwo words x and y as\\n\\x0c                                  Language is never, ever, ever, random   267\\n\\n  I (x; y) ⫽ log 2   冉    p (xandy)\\n                         p (x) · p(y)\\n                                        冊\\nand then estimate probabilities directly from frequencies, that is using\\nthe ‘Maximum Likelihood Estimate’ (MLE) of f (x)/N for p (x), f (y)/N\\nfor p (y), f (x-and-y)/N for p (x-and-y), thereby giving\\n\\n\\n  I (x; y) ⫽ log 2   冉   N · f (x ⫺ and ⫺ y)\\n                              f (x) · f (y)\\n                                               冊\\n   Dunning (1993) presents a critique of the use of Mutual Information\\nin empirical linguistics. His objection has been confused with the critique\\nof hypothesis-testing I make here so I mention his work in order to\\nclarify that the two objections, while both valid, are different in nature\\nand independent.\\n   Dunning demonstrates how MLEs fare poorly when estimating the\\nprobabilities of rare events. The problem is essentially this: if a word (or\\nbigram, or trigram, or character-sequence etc.) occurs just once or twice\\nin a corpus of N words (bigrams, etc.), then the simplest way to estimate\\nthe probability is the NILE, which gives 1/N or 2/N. However this does\\nnot factor in the arbitrariness of the word occurring at all in the corpus:\\nin a corpus ten times the size, there would be roughly ten times the\\nnumber of singletons and doubletons in the corpus, most of which would\\nnot have occurred at all in the original corpus. Thus some of the prob-\\nability mass contributing to the 1/N or 2/N MLEs should have been put\\naside for the words (bigrams etc.) which did not occur at all in this\\nparticular corpus. Viewed another way, the 1/N and 2/N should be dis-\\ncounted to allow for the fact that one or two occurrences are very low\\nbases of evidence on which to assert probabilities.\\n   There are various ways in which the discounting can be done, for\\nexample the Good-Turing method (Good 1953), usefully applied to em-\\npirical linguistics in Gale and Sampson (1995), Bod (1995). Dunning\\npresents and advocates the use of the log-likelihood statistic, which, like\\nthe χ2 statistic, is χ2-distributed,4 but more accurately estimates probabil-\\nities where counts are low. The log-likelihood statistic still only estimates\\nprobabilities: since Dunning’s work, Pedersen (1996) has shown how\\nFisher’s Exact Method can be applied to the problem, to identify the\\nexact probability of a word (bigram etc.) rather than estimating it at all.\\n   Thus Dunning’s objection to Mutual Information is that it fails to\\naccurately represent probabilities when counts are low (where ‘low’ is\\ngenerally taken as less than five). If the probabilities can be accurately\\nrepresented, Dunning’s anxieties will be set at ease.\\n\\x0c268    A. Kilgarriff\\n\\n   The critique in this paper does not concern whether probabilities are\\naccurately calculated. Rather, the objection is that the probability model,\\nwith its assumptions of randomness, is inappropriate, particularly where\\ncounts are high (eg, thousands or more).\\n   Where the task is to determine whether there is an interesting associa-\\ntion between two rare events, Dunning’s concern must be heeded. Where\\nit is to determine whether there is an interesting association between\\nhigh-frequency events, the concerns of this paper must be.\\n\\n\\n4. Experiment\\nGiven enough data, H0 is almost always rejected however arbitrary the\\ndata, as the author discovered when grappling with the following data.\\n   Two corpora were set up to be indisputably of the same language type,\\nwith only arbitrary differences between them: each was a random subset\\nof the written part of the British National Corpus (BNC). The sampling\\nwas as follows: all texts shorter than 20,000 words were excluded. This\\nleft 820 texts. Half the texts were then randomly assigned to each of\\ntwo corpora.\\n   The null hypotheses were (1) that the two subcorpora, viewed as col-\\nlections of words rather than documents, were random samples drawn\\nfrom the same population; and consequently, (2) that the deviation in\\nfrequency of occurrence for each individual word between the two sub-\\ncorpora was explicable as random fluctuation. The HO were tested using\\nthe χ2-test: is χ2\\n\\n  Σ(|O ⫺ E | ⫺ 0.5) 2 /E\\n\\ngreater than the critical value? The sum is over the four cells of the\\ncontingency table\\n\\n                Corpus 1    Corpus 2\\n\\nword w          a           b\\nnot word w      c           d\\n\\n\\n  If we randomly assign words (as opposed to documents) to the one\\ncorpus or the other, then we have a straightforward random distribution,\\nwith the value of the χ2-statistic equal to or greater than the 99.5 %\\nconfidence threshold of 7.88 for just 0.5 % of words. The average value\\nof the error term,\\n\\x0c                                Language is never, ever, ever, random          269\\n\\n  (|O ⫺ E| ⫺ 0.5) 2 /E\\n\\nis then 0.5.5 The hypothesis can, therefore, be couched as: are the error\\nterms systematically greater than 0.5? If they are, we should be wary of\\nattributing high error terms to significant differences between text types,\\nsince we also obtain many high error terms where there are no significant\\ndifferences between text types.\\n   Frequency lists for word-POS pairs for each subcorpus were gener-\\nated. For each word occurring in either subcorpus, the error term which\\nwould have contributed to a χ2 calculation was determined. As Table 4\\nshows, average values for the error term are far greater than 0.5, and\\ntend to increase as word frequency increases.\\n   As the averages indicate, the error term is very often greater than 0.5\\n* 7.88 ⫽ 3.94, the relevant critical value of the chi-square statistic. For\\nvery many words, including most common words, the null hypothesis is\\nresoundingly defeated (as is the null hypthesis regarding the two subc-\\norpora as wholes).\\n   There is no a priori reason to expect words to behave as if they had\\nbeen selected at random, and indeed they do not. It is in the nature of\\n\\n\\n\\n\\nTable 1. Comparing two same-genre corpora using χ2\\nClass                       First item in class                  Mean error term\\n(Words in freq. order)                                           for items in class\\n                            word                  POS\\n\\nFirst 10 items              the                   DET            18.76\\nNext 10 items               for                   PRP            17.45\\nNext 20 items               not                   XX             14.39\\nNext 40 items               have                  VHB            10.71\\nNext 80 items               also                  AVO             7.03\\nNext 160 items              know                  VVI             6.40\\nNext 320 items              six                   CRD             5.30\\nNext 640 items              finally               AV0             6.71\\nNext 1280 items             plants                NN2             6.05\\nNext 2560 items             pocket                NN1             5.82\\nNext 5120 items             represent             VVB             4.53\\nNext 10240 items            peking                NP0             3.07\\nNext 20480 items            fondly                AV0             1.87\\nNext 40960 items            chandelier            NN1             1.15\\nTable note. Mean error term is far greater than 0.5, and increases with frequency.\\nPOS tags are drawn from the CLAWS-5 tagset as used in the BNC (see http:/info.ox.\\nac.uk/bnc)\\n\\x0c270     A. Kilgarriff\\n\\nlanguage that any two collections of texts, covering a wide range of\\nregisters (and comprising, say, less than a thousand samples of over a\\nthousand words each) will show such differences. While it might seem\\nplausible that oddities would in some way balance out to give a popula-\\ntion that was indistinguishable from one where the individual words (as\\nopposed to the texts) had been randomly selected, this turns out not to\\nbe the case.\\n   The key word in the last paragraph is ‘indistinguishable’. In hypothesis\\ntesting, the objective is generally to see if the population can be distin-\\nguished from one that has been randomly generated ⫺ or, in our case,\\nto see if the two populations are distinguishable from two populations\\nwhich have been randomly generated on the basis of the frequencies in\\nthe joint corpus. Since words in a text are not random, we know that\\nour corpora are not randomly generated, and the hypothesis test con-\\nfirms the fact.\\n\\n\\n5.    Re-analysis of previous work\\n\\n5.1. Brown and LOB\\n\\nHofland and Johansson (1982) wanted to find words which were signifi-\\ncantly different in their frequencies between British and American Eng-\\nlish, as represented in the Brown corpus for American English and LOB\\ncorpus for British. For each word, they tested the null hypothesis that\\nthe difference in frequency between the two corpora could be explained\\nas random variation, with the samples being random samples from the\\nsame source, and in their frequency lists, they mark words where the\\nnull hypothesis was defeated (at a 95, 99 or 99.9 % confidence level).\\nLooking at these lists suggests that virtually all common words are\\nmarkedly different in their levels of use between the US and the UK:\\nthey are all marked as such. By contrast, most of the rarer marked words\\nare words we know to be American or British, or to refer to items that\\nare more common or more salient in the US or the UK.\\n   As the argument of the previous section explains, most of the marked\\nhigh-frequency words are marked simply as a consequence of the essen-\\ntially non-random nature of language. It would not be surprising for a\\nhigh-frequency word marked as British English in these lists to be\\nmarked as American English in a repeat of the experiment using new\\ndata.\\n   Similar strategies are used by, and a similar critique is applicable to,\\nLeech and Fallon (1992) (again, for comparing LOB and Brown), Ray-\\n\\x0c                              Language is never, ever, ever, random    271\\n\\nson, Leech, and Hodges (1997) for comparing the conversation of dif-\\nferent social groups, and Rayson and Garside (2000) for contrasting the\\nlanguage of a specialist genre with ‘general language’, as represented by\\nthe British National Corpus.\\n\\n\\n5.2 Subcategorization frame (SCF) learning\\nHypothesis-testing has been used in a number of papers concerning the\\nautomatic acquisition of subcategorization frames (SCFs) for verbs from\\ncorpora. The problem is this. Dictionaries, even where they do present\\nexplicit and accurate SCFs for verbs, are not complete: they do not pre-\\nsent all the frames for each verb. This gives rise to many parsing errors.\\nResearchers including Brent (1993), Briscoe and Carroll (1997) and Kor-\\nhonen (2000) have developed methods for SCF acquisition. However,\\ntheir methods are inevitably noisy, suffering, for example, from just those\\nparser errors that the whole process is designed to address, and they do\\nnot wish to accept any SCF for which there is any evidence as a true\\nSCF for the verb. They wish to filter out those SCFs where the evidence\\nis not strong enough. Brent and Briscoe and Carroll used hypothesis\\ntesting to this end. However, problems are noted:\\n\\n  Further evaluation of the results ...reveals that the filtering phase is\\n  the weak link in the system … The performance of the filter for classes\\n  with less than 10 exemplars is around chance, and a simple heuristic\\n  of accepting all classes with more then 10 exemplars would have pro-\\n  duced broadly similar results for these verbs (Briscoe and Carroll\\n  1997: 360⫺36).\\n\\nKorhonen, Correll, and McCarthy (2000) explore the issue in detail.\\nUsing Briscoe and Carroll’s SCF acquisition system, they explore the\\nimpact of four different strategies for filtering out noise:\\n\\nBaseline    No filter\\nBHT         binomial hypothesis test: reject the SCF if Ho is not defeated6\\nLLR         hypothesis test using log-likelihood ratio: reject the SCF if\\n            Ho is not defeated\\nMLE         threshold based on the relative frequency (which is also the\\n            maximum likelihood estimate (MLE) of the probability) of\\n            the verb occurring in the SCF given the verb, with the thresh-\\n            old determined empirically\\n\\x0c272    A. Kilgarriff\\n\\n  They observe\\n\\n  MLE thresholding produced better results than the two statistical tests\\n  used. Precision improved considerably, showing that the classes occur-\\n  ring in the data with the highest frequency are often correct … MLE\\n  is not adept at finding low frequency SCFs … (Korhonen, Correll,\\n  and McCarthy 2000: 202)\\n\\nThis concurs with the theoretical argument above. Hypothesis tests are\\ninappropriate for the task, because the relations between verb and SCF\\nwill never be random and the hypothesis test will merely reject the null\\nhypothesis wherever there is enough data, in a manner not closely corre-\\nlated with whether the SCF-verb link is motivated. Where there is\\nenough data, then the relationship between verb and SCF is easy to see\\nso even a simple threshold method will identify the verb’s SCFs. Where\\ndata is very sparse, no method works well.\\n   Korhonen (2000) extends this line of work, exploring thresholding\\nmethods where a more accurate estimate of the probability is obtained\\nby using data from semantically similar but higher frequency verbs. She\\nachieves modest improvements over the baseline which uses Korhonen,\\nCorrell and McCarthy’s MLE, particularly when combining the fre-\\nquencies of the target verb and its semantic neighbour using a linear\\nmethod based on the quantity of evidence available for each.\\n   The problem is not one of distinguishing random and non-random\\nrelationships, but of sparseness of data. Where the data is not sparse,\\nthe difference between arbitrary and motivated connections is evident in\\ngreatly differing relative frequencies. This makes the moral of the story\\nplain. Data is abundant. A modest-frequency verb like devastate occurs\\n(Google tells us) in well over a million web pages. With just 1 % of them,\\ndevastate becomes one of the verbs for which we have plenty of data,\\nand crude thresholding methods will distinguish associated SCFs from\\nnoise. It is possible that parsing errors are systematic and thus that the\\nsame errors occur very often in very large corpora although our experi-\\nence from looking at large corpora in the Word Sketch Engine (Kilgarriff\\net al 2004) suggests not. Harvesting the web (or other huge corpora) is\\nthe way to build an accurate SCF lexicon.7\\n\\n6. Conclusion\\nLanguage is non-random and hence, when we look at linguistic phenom-\\nena in corpora, the null hypothesis will never be true. Moreover, where\\nthere is enough data, we shall (almost) always be able to establish that\\nit is not true. In corpus studies, we frequently do have enough data, so\\n\\x0c                                    Language is never, ever, ever, random               273\\n\\nthe fact that a relation between two phenomena is demonstrably non-\\nrandom, does not support the inference that it is not arbitrary. Hypoth-\\nesis testing is rarely useful for distinguishing associated from non-associ-\\nated pairs of phenomena in large corpora. Where used, it has often led\\nto unhelpful or misleading results.\\n   Hypothesis testing has been used to reach conclusions, where the diffi-\\nculty in reaching a conclusion is caused by sparsity of data. But language\\ndata, in this age of information glut, is available in vast quantities. A\\nbetter strategy will generally be to use more data Then the difference\\nbetween the motivated and the arbitrary will be evident without the use\\nof compromised hypothesis testing. As Lord Rutherford put it: “If your\\nexperiment needs statistics, you ought to have done a better experi-\\nment.”\\nReceived July 2004                                            Lexical Computing Ltd.\\nRevisions received May 2005\\nFinal acceptance May 2005\\n\\nNotes\\n  * This work was supported by the UK EPSRC, under grants GR/K18931 (SEAL)\\n    and GR/M54971 (WASPS).\\n 1. In this paper we do not consider the distinction between the predictable and the\\n    ‘merely’ motivated.\\n 2. Diapers, in American English\\n 3. There is some confusion over names. In information theory, Mutual Information\\n    is usually defined over a whole population of words, rather than being specified\\n    for a particular word-pair, as here, and the definition incorporates information\\n    from all cells of the contingency table. Church and Hanks only use a subset of that\\n    information. Church-and-Hanks Mutual Information has been called Pointwise\\n    Mutual Information. see Manning and Schütze (1999: 66 ff.) for a fuller discus-\\n    sion. Here we use Church and Hanks’s definition and name.\\n 4. This sentence will be confusing to non-mathematicians. The χ2 statistic is a statis-\\n    tic, that is, it can be calculated from a data sample using actual numbers. The χ2\\n    distribution is a theoretical construct. If a sufficiently large number of chi-square\\n    statistics are calculated, all from true random samples of the same population,\\n    then this population of χ2 statistics will, provably, fit a χ2 distribution. This is\\n    also true for other statistics: that is, if a sufficiently large number of log-likelihood\\n    statistics are calculated, all from true random samples of the same population,\\n    then this population of log-likelihood statistics will, provably, fit a χ2 distribution.\\n    Some texts call the statistic χ2 rather than χ2 to distinguish it more clearly from\\n    the distribution, but this practice is in the minority and is not adopted here.\\n 5. See appendix\\n 6. The model used was a sophisticated one incorporating evidence about type fre-\\n    quencies of verbs from the ANLT lexicon: see Briscoe and Carroll (1997) or Kor-\\n    honen, Correll, and McCarthy (2000) for details.\\n 7. See Kilgarriff and Grefenstette (2003) and papers therein. The web is a vast re-\\n    source for many languages. See also Banko and Brill (2001) for the benefits of\\n    large data over sophisticated mathematics.\\n\\x0c274     A. Kilgarriff\\n\\nAppendix\\nThe average value of the error term is 0.5. We explain this as follows.\\n  If we do in fact have a random distribution, then by the definition of\\nthe χ2 distribution, the sum of the cells in the contingency table is 1:\\n\\n  a⫹b⫹c⫹d⫽1\\n\\nEach of these error terms is calculated as\\n\\n  (O ⫺ E ⫺ 0.5) 2/E\\n\\nIn our situation, there are very large datasets and the phenomenon of\\ninterest only accounts for a very small proportion of cases. The fre-\\nquency of not word w is very high. Thus the expected values, E, for not\\nword w to be used when calculating c and d for the contingency table are\\nvery high. As we divide by very large E, c and d are vanishingly small, so\\n\\n  a⫹b⫹c⫹d⫽1\\n\\nreduces to\\n\\n  a⫹b⫽1\\n\\nSince we have set the situation up symmetrically, a and b are the same\\nsize, so each will be, on average, 0.5.\\n\\nReferences\\nBanko, Michele and Eric Brill\\n  2001       Scaling to very very large corpora for natural language disambiguation.\\n             Proceedings of the 39th Annual Meeting of the Association for Computa-\\n             tional Linguistics and the 10th Conference of the European Chapter of the\\n             Association for Computational Linguistics.\\nBod, Rens\\n  1995       Enriching linguistics with statistics: performance models of natural lan-\\n             guage. Ph.D. dissertation, University of Amsterdam.\\nBrandstätter, E.\\n  1999       Confidence intervals as an alternative to significance testing. Methods of\\n             Psychological Research Outline 4(2), 33⫺46.\\nBrent, Michael R.\\n  1993       From grammar to lexicon: unsupervised learning of lexical syntax. Com-\\n             putational Linguistics 19(2), 243⫺262.\\nBriscoe, Ted and John Carroll\\n  1997       Automatic extraction of subcategorization from corpora. Proceedings of\\n             the Fifth Conference on Applied Natural Language Processing, 356⫺363.\\n\\x0c                                    Language is never, ever, ever, random            275\\nCarver, R. P.\\n  1993        The case against statistical significance testing, revisited. Journal of Ex-\\n              perimental Education 61, 287⫺292.\\nChurch, Kenneth and Patrick Hanks\\n  1990        Word association norms, mutual information and lexicography. Compu-\\n              tational Linguistics 16(1), 22⫺29.\\nDunning, Ted\\n  1993        Accurate methods for the statistics of surprise and coincidence. Compu-\\n              tational Linguistics 19(1), 61⫺74.\\nGale, William and Geoffrey Sampson\\n  1995        Good-Turing frequency estimation without tears. Journal of Quantitative\\n              Linguistics 2(3),\\nGood, I. J.\\n  1953        The population frequencies of species and the estimation of population\\n              parameters. Biometrika 40, 237⫺264.\\nGrefenstette, Gregory and Julien Nioche\\n  2000        Estimation of English and non-English language use on the www. In\\n              Proceedings of RIAO (Recherche d’Informations Assiste´ e par Ordinateur),\\n              237⫺246.\\nHofland, Knud and Stig Johanson (Eds.)\\n  1982        Word Frequencies in British and American English. Bergen: The Norwe-\\n              gian Computing Centre for the Humanities.\\nKorhonen, Anna\\n  2000        Using semantically motivated estimates to help subcategorization\\n              acquisition. Proceedings of the Joint Conference on Empirical Methods in\\n              NLP and Very Large Corpora, 216⫺223.\\nKorhonen, Anna, Genevieve Gorrell, and Diana McCarthy\\n  2000        Statistical filtering and subcategorization frame acquisition. Proceedings\\n              of the Joint Conference on Empirical Methods in NLP and Very Large\\n              Corpora, 199⫺206.\\nLDOCE\\n  1995        Longman Dictionary of Contemporary English, 3rd Edition. Ed. Della\\n              Summers. Harlow: Longman.\\nLeech, Geoffrey and Roger Fallon\\n  1992        Computer corpora — what do they tell us about culture? ICAME Journal\\n              16, 29⫺50.\\nManning, Christopher and Hinrich Schütze\\n  1999        Foundations of Statistical Natural Language Processing. Cambridge, MA:\\n              MIT Press.\\nOwen, Frank and Ronald Jones\\n  1977        Statistics. Polytech Publishers.\\nPedersen, Ted\\n  1996        Fishing for exactness. Proceedings of the Conference of the South-Central\\n              SAS Users Group, 188⫺200.\\nRayson, Paul and Roger Garside\\n  2000        Comparing corpora using frequency profiling. Proceedings of the Work-\\n              shop on Comparing Corpora, 38th ACL, 1⫺6.\\nRayson, Paul, Geoffrey Leech, and Mary Hodges\\n  1997        Social differentiation in the use of English vocabulary: some analysis of\\n              the conversational component of the British National Corpus. Interna-\\n              tional Journal of Corpus Linguistics 2(1), 133⫺152.\\nStubbs, Michael\\n  1995        Collocations and semantic profiles: On the cause of the trouble with\\n              quantitative studies. Functions of Language 2(1), 23⫺55.\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "# Tokenize the text.\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
        "                  for sent in sent_tokenize(text)]\n",
        "\n",
        "tokenized_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_RcVK2jqKEh",
        "outputId": "93f38f06-75f9-4662-cb72-b075014e52dc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language',\n",
              " 'is',\n",
              " 'never',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'random',\n",
              " 'adam',\n",
              " 'kilgarriff',\n",
              " 'abstract',\n",
              " 'language',\n",
              " 'users',\n",
              " 'never',\n",
              " 'choose',\n",
              " 'words',\n",
              " 'randomly',\n",
              " ',',\n",
              " 'and',\n",
              " 'language',\n",
              " 'is',\n",
              " 'essentially',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHGYYq-bqOoe",
        "outputId": "9bd36982-7d81-4755-e926-d3eb16c09b32"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-gram 모델 선정"
      ],
      "metadata": {
        "id": "8E103domqcIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for 3-grams language modelling\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6GZBwzoqZ3S",
        "outputId": "92b9e9df-c5fb-4e9f-9999-6722e096e5ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x7fdd0974b530>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습을 위해 MLE(Maximum Likelihood Estimation) 추정"
      ],
      "metadata": {
        "id": "w_y-qLu8qrey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
      ],
      "metadata": {
        "id": "VX0iV3hgqfc0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvsnJk-_qx8O",
        "outputId": "9756e942-29a4-4225-9949-7b99408cb66a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciqL1xVHq0m8",
        "outputId": "62e95ee7-54b9-4724-bcf4-07db494a687e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vocab.lookup(tokenized_text[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4JXdRW3q-rk",
        "outputId": "3d5b240c-a54e-4aec-8984-7046a0fe6448"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 만약 Vocab 집합에 포함되지 않는 단어라면 <UNK>라는 특수 토큰으로 처리됩니다."
      ],
      "metadata": {
        "id": "q-VscLaTrJdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we lookup the vocab on unseen sentences not from the training data,\n",
        "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
        "print(model.vocab.lookup('language is never random nsfkalfnsaklfsankllah .'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Wc-wudrHNq",
        "outputId": "d7501878-64f4-4dd8-d017-95f11767daaf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('language', 'is', 'never', 'random', '<UNK>', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yPpprQdrMZU",
        "outputId": "f692e372-20f9-4afc-a1fc-203c8033719a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 19611 ngrams>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# language 단어가 몇 번 나타났나? 25번\n",
        "model.counts['language'] # i.e. Count('language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHKwEXUIrPzv",
        "outputId": "7d8528af-a6ff-4832-8f64-802e6ebd784f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# language 단어가 등장한 뒤에 이어서 is 단어가 나온 횟수? 11번\n",
        "model.counts[['language']]['is'] # i.e. Count('is'|'language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ1Mo3hYrRYC",
        "outputId": "1c249fd7-de04-477c-819e-da48f1e78ed9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# language, is 조합 이후에 never 단어가 나온 횟수? 7번\n",
        "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IlbK6E2rThh",
        "outputId": "0be61feb-f3bf-4dce-8980-16d7a493ab73"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('language') # P('language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2gL9zlqrU3-",
        "outputId": "c02994e5-84c3-4085-f5ac-34359089c3bb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003691671588895452"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('is', 'language'.split())  # P('is'|'language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUPrzhzeryzd",
        "outputId": "a273706b-bf76-4726-8228-2e1887e3250b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score('never', 'language is'.split())  # P('never'|'language is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMogGuxcr0VU",
        "outputId": "c017b19f-390e-47f0-d618-f7dd1912ba20"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363636363636364"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unknown 같은 경우 vocab 포함 안되는 경우 = 확률 0\n",
        "print(model.score(\"<UNK>\") == model.score(\"lah\"))\n",
        "print(model.score(\"<UNK>\"))\n",
        "print(model.score(\"lah\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcLcV7_Dr7f2",
        "outputId": "194ca1f4-90d2-449b-f4d3-4c67d67b9902"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(\"<UNK>\") == model.score(\"nsfkalfnsaklfsankllah\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC58t05zr8x8",
        "outputId": "2667da7d-40a0-412a-80ee-942d613fc0aa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(\"<UNK>\") == model.score(\"lor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsEs5s94sC1a",
        "outputId": "9b5df188-0a89-4e78-fa6c-962675d76aba"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 편의를 위해 log를 씌운 확률로도 계산할 수 있음\n",
        "model.logscore(\"never\", \"language is\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWWjqhuGsGa-",
        "outputId": "b449c795-eac3-49db-edce-c22aabecc14b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6520766965796932"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram 모델을 이용해서 랜덤한 새로운 텍스트를 생성합니다.\n"
      ],
      "metadata": {
        "id": "Xsnn66JqsNzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate(20, random_seed=7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq1_s-ussK3-",
        "outputId": "6a10e2b7-5a1f-44eb-83d6-83c42262048c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'carroll', 'used', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'a', 'half', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "metadata": {
        "id": "UF2og1v-sQzR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 20, random_seed=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JDodPY7XsSGV",
        "outputId": "46e1d07e-72c7-4a81-f544-6024d347b905"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'and carroll used hypothesis testing has been used, and a half.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate(28, random_seed=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKbSEVvCsb22",
        "outputId": "1a06c29c-9cc2-44d9-a4c1-a5fd75016e4d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'scf-verb', 'link', 'is', 'motivated', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 28, random_seed=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WndRDwrxseNb",
        "outputId": "d2f38d53-24ca-4d03-bc31-e122ebee4566"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the scf-verb link is motivated.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 20, random_seed=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hco3MgnMsf32",
        "outputId": "713f277a-0f97-4d02-fbb5-ddced1d3d425"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'237⫺246.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 20, random_seed=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r7E5TTz2sitp",
        "outputId": "c7398b5e-f45d-4d28-c306-2ab25584460a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hypothesis is ever a useful construct.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 20, random_seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b4XOCW9jskAV",
        "outputId": "fa24e604-c245-4e83-e38e-2e23befe51e2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'more (or cold) weather, or on saturday nights, or by people in (or poorer)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RW4rtg9Pslki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}